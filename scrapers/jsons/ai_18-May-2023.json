[{"Category": "Interviews", "Title": "Razi Raziuddin, Co-Founder & CEO of FeatureByte \u2013 Interview Series", "URL": "https://www.unite.ai/razi-raziuddin-co-founder-ceo-of-featurebyte-interview-series/", "Date": "2023-05-08T17:48:01-04:00", "Description": "Razi Raziuddin is the Co-Founder & CEO of FeatureByte, his vision is to unlock the last major hurdle to scaling AI in the enterprise.\u00a0 Razi\u2019s analytics and growth experience spans the leadership team of two unicorn startups. Razi helped scale DataRobot from 10 to 850 employees in under six years. He pioneered a services-led go-to-market [\u2026]", "Body": "Razi Raziuddin\n is the Co-Founder & CEO of \nFeatureByte\n, his vision is to unlock the last major hurdle to scaling AI in the enterprise.\u00a0 Razi\u2019s analytics and growth experience spans the leadership team of two unicorn startups. Razi helped scale DataRobot from 10 to 850 employees in under six years. He pioneered a services-led go-to-market strategy that became the hallmark of DataRobot\u2019s rapid growth.\nFeatureByte is on a mission to scale enterprise AI, by radically simplifying and industrializing AI data. The feature engineering and management (FEM) platform empowers data scientists to create and share state-of-the-art features and production-ready data pipelines in minutes \u2014 instead of weeks or months.\nWhat initially attracted you to computer science and \nmachine learning\n?\nAs someone who started coding in high school, I was fascinated with a machine that I could \u201ctalk\u201d to and control through code. I was instantly hooked on the endless possibilities of new applications. Machine learning represented a paradigm shift in programming, allowing machines to learn and perform tasks without even specifying the steps in code. The infinite potential of ML applications is what gets me excited every day.\nYou were the first business hire at DataRobot, an automated machine learning platform that enables organizations to become AI driven. You then helped to scale the company from 10 to 1,000 employees in under 6 years. What were some key takeaways from this experience?\nGoing from zero to one is hard, but incredibly exciting and rewarding. Each stage in the company\u2019s evolution presents a different set of challenges, but seeing the company grow and succeed is an amazing feeling.\nMy experience with AutoML opened my eyes to the unbounded potential of AI. It's fascinating to see how this technology can be used across so many different industries and applications. At the end of the day, creating a new category is a rare feat, but an incredibly rewarding one. My key takeaways from the experience:\nBuild an amazing product and avoid chasing fads\nDon\u2019t be afraid to be a contrarian\nFocus on solving customer problems and providing value\nAlways be open to innovation and trying new things\nCreate and inculcate the right company culture from the very start\nCould you share the genesis story behind FeatureByte?\nIt's a well-known fact in the AI/ML world \u2013 that Great AI starts with great data. But preparing, deploying and managing AI data (or Features) is complex and time-consuming. My co-founder, Xavier Conort, and I saw this problem firsthand at DataRobot. While modeling has become vastly simplified thanks to AutoML tools, feature engineering and management remains a huge challenge. Based on our combined experience and expertise, Xavier and I felt we could truly help organizations solve this challenge and deliver on the promise of AI everywhere.\nFeature engineering is at the core of FeatureByte, could you explain what this is for our readers?\nUltimately, the quality of data drives the quality and performance of AI models. Data that is fed into models to train them and predict future outcomes is called Features. Features represent information about entities and events, such as demographic or psychographic data of consumers, or distance between a cardholder and merchant for a credit card transaction or number of items of different categories from a store purchase.\nThe process of transforming raw data into features \u2013 to train ML models and predict future outcomes \u2013 is called feature engineering.\nWhy is feature engineering one of the most complicated aspects of machine learning projects?\nFeature engineering is super important because the process is directly responsible for the performance of ML models. Good feature engineering requires three fairly independent skills to come together \u2013 domain knowledge, data science and data engineering. Domain knowledge helps data scientists determine what signals to extract from the data for a particular problem or use case. You need data science skills to extract those signals. And finally, data engineering helps you deploy pipelines and perform all those operations at scale on large data volumes.\nIn the vast majority of organizations, these skills live in different teams. These teams use different tools and don\u2019t communicate well with each other. This leads to a lot of friction in the process and slows it down to a grinding halt.\nCould you share some insight on why feature engineering is the weakest link in scaling AI?\nAccording to Andrew Ng, renowned expert in AI, \u201cApplied machine learning is basically feature engineering.\u201d Despite its criticality to the machine learning lifecycle, feature engineering remains complex, time consuming and dependent on expert knowledge. There is a serious dearth of tools to make the process easier, quicker and more industrialized. The effort and expertise required holds enterprises back from being able to deploy AI at scale.\nCould you share some of the challenges behind building a data-centric AI solution that radically simplifies feature engineering for data scientists?\nBuilding a product that has a 10X advantage over the status quo is super hard. Thankfully, Xavier has deep data science expertise that he\u2019s employing to rethink the entire feature workflow from first principles. We have a world-class team of full-stack data scientists and engineers who can turn our vision into reality. And users and development partners to advise us on streamlining the UX to best solve their challenges.\nHow will the FeatureByte platform speed up the preparation of data for machine learning applications?\nData preparation for ML is an iterative process that relies on rapid experimentation. The open source FeatureByte SDK is a declarative framework for creating state-of-the-art features with just a few lines of code and deploying data pipelines in minutes instead of weeks or months. This allows data scientists to focus on creative problem solving and iterating rapidly on live data, rather than worrying about the plumbing.\nThe result is not only faster data preparation and serving in production, but also improved model performance through powerful features.\nCan you discuss how the FeatureByte platform will additionally offer the ability to streamline various ongoing management tasks?\nThe FeatureByte platform is designed to manage the end-to-end ML feature lifecycle. The declarative framework allows FeatureByte to deploy data pipelines automatically, while extracting metadata that is relevant to managing the overall environment. Users can monitor pipeline health and costs, and manage the lineage, version and correctness of features all from the same GUI. Enterprise-grade role-based access and approval workflows ensure data privacy and security, while avoiding feature sprawl.\nIs there anything else that you would like to share about FeatureByte?\nMost enterprise AI tools focus on improving machine learning models. We've made it a mission to help enterprises scale their AI, by simplifying and industrializing AI data. At FeatureByte, we address the biggest challenge for AI practitioners: Providing a consistent, scalable way to prep, serve and manage data across the entire lifecycle of a model, while radically simplifying the entire process.\nIf you\u2019re a data scientist or engineer interested in staying at the cutting edge of data science, I\u2019d encourage you to experience the power of \nFeatureByte for free\n.\nThank you for the great interview, readers who wish to learn more should visit \nFeatureByte\n."}, {"Category": "Interviews", "Title": "Arjun Narayan, Head of Global Trust and Safety for SmartNews \u2013 Interview Series", "URL": "https://www.unite.ai/arjun-narayan-head-of-global-trust-and-safety-for-smartnews-interview-series/", "Date": "2023-05-12T12:43:59-04:00", "Description": "Arjun Narayan, is the Head of Global Trust and Safety for SmartNews a news aggregator app, he is also an AI ethics, and tech policy expert.\u00a0 SmartNews uses AI and a human editorial team as it aggregates news for readers. You were instrumental in helping to Establish Google's Trust & Safety Asia Pacific hub in [\u2026]", "Body": "Arjun Narayan\n, is the Head of Global Trust and Safety for \nSmartNews\n a news aggregator app, he is also an AI ethics, and tech policy expert.\u00a0 SmartNews uses AI and a human editorial team as it aggregates news for readers.\nYou were instrumental in helping to Establish Google's Trust & Safety Asia Pacific hub in Singapore, what were some key lessons that you learned from this experience?\nWhen building Trust and Safety teams country-level expertise is critical because abuse is very different based on the country you\u2019re regulating. For example, the way Google products were abused in Japan was different than how they were abused in Southeast Asia and India. This means abuse vectors are very different depending on who's abusing, and what country you're based in; so there's no homogeneity. This was something we learned early.\nI also learned that cultural diversity is incredibly important when building Trust and Safety teams abroad. At Google, we ensured there was enough cultural diversity and understanding within the people we hired. We were looking for people with specific domain expertise, but also for language and market expertise.\nI also found cultural immersion to be incredibly important. When we\u2019re building Trust and Safety teams across borders, we needed to ensure our engineering and business teams could immerse themselves. This helps ensure everyone is closer to the issues we were trying to manage.\u00a0 To do this, we did quarterly immersion sessions with key personnel, and that helped raise everyone\u2019s cultural IQs.\nFinally, cross-cultural comprehension was so important. I managed a team in Japan, Australia, India, and Southeast Asia, and the way in which they interacted was wildly different. As a leader, you want to ensure everyone can find their voice. Ultimately, this is all designed to build a high-performance team that can execute sensitive tasks like Trust and Safety.\nPreviously, you were also on the Trust & Safety team with ByteDance for the TikTok application, how are videos that are often shorter than one minute monitored effectively for safety?\nI want to reframe this question a bit, because it doesn\u2019t really matter whether a video is short or long form. That isn\u2019t a factor when we evaluate video safety, and length doesn\u2019t have real weight on whether a video can spread abuse.\nWhen I think of abuse, I think of abuse as \u201cissues.\u201d What are some of the issues users are vulnerable to? Misinformation? Disinformation? Whether that video is 1 minute or 1 hour, there is still misinformation being shared and the level of abuse remains comparable.\nDepending on the issue type, you start to think through policy enforcement and safety guardrails and how you can protect vulnerable users. As an example, let's say there's a video of someone committing self-harm. When we receive notification this video exists, one must act with urgency, because someone could lose a life. We depend a lot on \nmachine learning\n to do this type of detection. The first move is to always contact authorities to try and save that life, nothing is more important. From there, we aim to suspend the video, livestream, or whatever format in which it is being shared. We need to ensure we\u2019re minimizing exposure to that kind of harmful content ASAP.\nLikewise, if it's hate speech, there are different ways to unpack that. Or in the case of bullying and harassment, it really depends on the issue type, and depending on that, we'd tweak our enforcement options and safety guardrails. Another example of a good safety guardrail was that we implemented machine learning that could detect when someone writes something inappropriate in the comments and provide a prompt to make them think twice before posting that comment. We wouldn\u2019t stop them necessarily, but our hope was that people would think twice before sharing something mean.\nIt comes down to a combination of machine learning and keyword rules. But, when it comes to livestreams, we also had human moderators reviewing those streams\u00a0 that were flagged by AI so they could report immediately and implement protocols. Because they\u2019re happening in real time, it\u2019s not enough to rely on users to report, so we need to have humans monitoring in real-time.\nSince 2021, you\u2019ve been the Head of Trust, Safety, and Customer experience at SmartNews, a news aggregator app. Could you discuss how SmartNews leverages machine learning and natural language processing to identify and prioritize high-quality news content?\nThe central concept is that we have certain \u201crules\u201d or machine learning technology that can parse an article or advertisement and understand what that article is about.\nWhenever there is something that violates our \u201crules\u201d, let's say something is factually incorrect or misleading, we have machine learning flag that content to a human reviewer on our editorial team. At that stage, a they understand our editorial values and can quickly review the article and make a judgement about its appropriateness or quality. From there, actions are taken to address it.\nHow does SmartNews use AI to ensure the platform is safe, inclusive, and objective?\nSmartNews was founded on the premise that hyper-personalization is good for the ego but is also polarizing us all by reinforcing biases and putting people in a filter bubble.\nThe way in which SmartNews uses AI is a little different because we\u2019re not exclusively optimizing for engagement. Our algorithm wants to understand you, but it's not necessarily hyper-personalizing to your taste. That\u2019s because we believe in broadening perspectives. Our AI engine will introduce you to concepts and articles beyond adjacent concepts.\nThe idea is that there are things people need to know in the public interest, and there are things people need to know to broaden their scope. The balance we try to strike is to provide these contextual analyses without being big-brotherly. Sometimes people won\u2019t like the things our algorithm puts in their feed. When that happens, people can choose to not read that article. However, we are proud of the AI engine\u2019s ability to promote serendipity, curiosity, whatever you want to call it.\nOn the safety side of things, SmartNews has something called a \u201cPublisher Score,\u201d this is an algorithm designed to constantly evaluate whether a publisher is safe or not. Ultimately, we want to establish whether a publisher has an authoritative voice. As an example, we can all collectively agree ESPN is an authority on sports. But, if you\u2019re a random blog copying ESPN content, we need to ensure that ESPN is ranking higher than that random blog. The publisher score also considers factors like originality, when articles were posted, what user reviews look like, etc. It\u2019s ultimately a spectrum of many factors we consider.\nOne thing that trumps everything is \u201cWhat does a user want to read?\u201d If a user wants to view clickbait articles, we won't stop them if it isn't illegal or breaks our guidelines. We don't impose on the user, but if something is unsafe or inappropriate, we have our due diligence before something hits the feed.\nWhat are your views on journalists using generative AI to assist them with producing content?\nI believe this question is an ethical one, and something we\u2019re currently debating here at SmartNews. How should SmartNews view publishers submitting content formed by generative AI instead of journalists writing it up?\nI believe that train has officially left the station. Today, journalists are using AI to augment their writing. It's a function of scale, we don't have the time in the world to produce articles at a commercially viable rate, especially as news organizations continue to cut staff. The question then becomes, how much creativity goes into this? Is the article polished by the journalist? Or is the journalist completely reliant?\nAt this juncture, generative AI is not able to write articles on breaking news events because there's no training data for it. However, it can still give you a pretty good generic template to do so. As an example, school shootings are so common, we could assume that generative AI could give a journalist a prompt on school shootings and a journalist could insert the school that was affected to receive a complete template.\nFrom my standpoint working with SmartNews, there are two principles I think are worth considering. Firstly, we want publishers to be up front in telling us when content was generated by AI, and we want to label it as such. This way when people are reading the article, they're not misled about who wrote the article. This is transparency at the highest order.\nSecondly, we want that article to be factually correct. We know that generative AI tends to make things up when it wants, and any article written by generative AI needs to be proofread by a journalist or editorial staff.\nYou\u2019ve previously argued for tech platforms to unite and create common standards to fight digital toxicity, how important of an issue is this?\nI believe this issue is of critical importance, not just for companies to operate ethically, but to maintain a level of dignity and civility. In my opinion, platforms should come together and develop certain standards to maintain this humanity. As an example, no one should ever be encouraged to take their own life, but in some situations, we find this type of abuse on platforms, and I believe that is something companies should come together to protect against.\nUltimately, when it comes to problems of humanity, there shouldn't be competition. There shouldn\u2019t even necessarily be competition on who is the cleanest or safest community\u2014we should all aim to ensure our users feel safe and understood. Let\u2019s compete on features, not exploitation.\nWhat are some ways that digital companies can work together?\nCompanies should come together when there are shared values and the possibility of collaboration. There are always spaces where there is intersectionality across companies and industries, especially when it comes to fighting abuse, ensuring civility in platforms, or reducing polarization. These are moments when companies should be working together.\nThere is of course a commercial angle with competition, and typically competition is good. It helps ensure strength and differentiation across companies and delivers solutions with a level of efficacy monopolies cannot guarantee.\nBut, when it comes to protecting users, or promoting civility, or reducing abuse vectors, these are topics which are core to us preserving the free world. These are things we need to do to ensure we protect what is sacred to us, and our humanity. In my opinion, all platforms have a responsibility to collaborate in defense of human values and the values that make us a free world.\nWhat are your current views on responsible AI?\nWe're at the beginning of something very pervasive in our lives. This next phase of generative AI is a problem that we don\u2019t fully understand, or can only partially comprehend at this juncture.\nWhen it comes to responsible AI, it\u2019s so incredibly important that we develop strong guardrails, or else we may end up with a Frankenstein monster of Generative AI technologies. We need to spend the time thinking through everything that could go wrong. Whether that is bias creeping into the algorithms, or large language models themselves being used by the wrong people to do nefarious acts.\nThe technology itself isn\u2019t good or bad, but it can be used by bad people to do bad things. This is why investing the time and resources in AI ethicists to do adversarial testing to understand the design faults is so critical. This will help us understand how to prevent abuse, and I think that\u2019s probably the most important aspect of responsible AI.\nBecause AI can\u2019t yet think for itself, we need smart people who can build these defaults when AI is being programmed. The important aspect to consider right now is timing \u2013 we need these positive actors doing these things NOW before it\u2019s too late.\nUnlike other systems we\u2019ve designed and built in the past, AI is different because it can iterate and learn on its own, so if you don\u2019t set up strong guardrails on what and how it\u2019s learning, we cannot control what it might become.\nRight now, we\u2019re seeing some big companies laying off ethics boards and responsible AI teams as part of major layoffs. It remains to be seen how seriously these tech majors are taking the technology and how seriously they\u2019re reviewing the potential downfalls of AI in their decision making.\nIs there anything else that you would like to share about your work with Smartnews?\nI joined SmartNews\u00a0 because I believe in its mission, the mission has a certain purity to it. I strongly believe the world is becoming more polarized, and there isn't enough media literacy today to help combat that trend.\nUnfortunately, there are too many people who take WhatsApp messages as gospel and believe them at face value. That can lead to tremendous consequences, including\u2014and especially\u2014violence. This all boils down to people not understanding what they can and cannot believe.\nIf we do not educate people, or inform them on how to make decisions on the trustworthiness of what they\u2019re consuming. If we do not introduce media literacy levels to discern between news and fake news, we'll continue to advocate the problem and increase the issues history has taught us not to do.\nOne of the most important components of my work at SmartNews is to help reduce polarization in the world. I want to fulfill the founder's mission to improve media literacy where they can understand what they're consuming and make informed opinions about the world and the many diverse perspectives.\nThank you for the great interview, readers who wish to learn more or want to try out a different type of news app should visit \nSmartNews\n."}]